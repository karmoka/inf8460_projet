{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "starter-kernel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzFDImXp17px",
        "colab_type": "text"
      },
      "source": [
        "# INF8460 - Inference in Natural Languages\n",
        "### Équipe 8, Membres: \n",
        "    - Raphael Croteau\n",
        "    - Francois Vidal\n",
        "    - Clément Maurel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUCw1PJD2UVx",
        "colab_type": "text"
      },
      "source": [
        "## Modèles Simples  \n",
        "\n",
        "! **Important** Ces modèles ne servent que de baseline. Notre 'vrai' modèle se trouve à la section \"BERT + SRL\"  \n",
        "\n",
        "Utilisation de modèles simples afin de déterminer la complexité du problème. Parmis ceux-ci, un MLP simple avec softmax ainsi qu'un MLP utilisant BERT pour faire ses embeddings (bert n'était _pas_ entraîné avec le modèle)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wosasmf2UUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import sklearn\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from nltk import TweetTokenizer\n",
        "from keras.utils import np_utils\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Flatten, Input\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Sequential\n",
        "from keras.models import Model\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wakFOZIk2UU1",
        "colab_type": "text"
      },
      "source": [
        "### Traiter et sauvegarder les plongements bert\n",
        "Ces plongements peuvent prendre longtemps à traiter. Une méthode que nous avons adoptée pour se simplifier la vie était de traiter les données par groupes de 10'000 qui sont enregistrés dans des fichiers temporaires. Ces fichiers sont ensuite concatener pour former un seul grand tableau."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIkwMyh-R7Cy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uncomment pour utiliser google drive. Sinon les fichiers doivent être dans le meme dossier que le notebook\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/inf8460_projet')\n",
        "# prefix = \"./inf8460_projet/My Drive/inf8460_projet/\"\n",
        "prefix = \"./\"\n",
        "\n",
        "# Lecture des données d'entraînement\n",
        "\n",
        "train_data = pd.read_csv(os.path.join(prefix, 'snli_train.csv'))\n",
        "train_data, validation_data = train_test_split(train_data, test_size=0.2)\n",
        "nd_documents = len(train_data) \n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvEpZq-_2UVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load une version pre-entrainé de bert\n",
        "nb_features = 768\n",
        "incremental_treatment = False # Permet le pretraitment indremental des données\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVnrAbIA2UVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_batch(data):\n",
        "  \"\"\" Entraine bert avec data et renvoie la moyenne des dernier 4 layers ainsi que les labels de ces donnees \"\"\"\n",
        "  length_data = len(data)\n",
        "  start = time.time()\n",
        "  embedded = torch.zeros([len(data), nb_features])\n",
        "  index = 0\n",
        "  with torch.no_grad():\n",
        "      for sent1, sent2 in zip(data['sentence1'], data['sentence2']):\n",
        "          sentence = \"[CLS] \" + sent1 + \" [SEP] \" + sent2\n",
        "          tokenized_sentence = tokenizer.tokenize(sentence)\n",
        "          sep_index = tokenized_sentence.index(\"[SEP]\") \n",
        "          ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_sentence)])\n",
        "          segments = torch.tensor([0]*sep_index + [1]*(len(tokenized_sentence) - sep_index))\n",
        "          encoded_layers, _ = model(ids, segments)\n",
        "\n",
        "          mean_layers = torch.zeros([5, 768])\n",
        "          for j in range(7, 12):\n",
        "              mean_layers[j-7] = torch.mean(encoded_layers[j], 1)\n",
        "          mean_layers = torch.mean(mean_layers, 0)\n",
        "          embedded[index] = mean_layers\n",
        "\n",
        "          if index % 1000 == 0:\n",
        "              print(f\"Document: {index}/{length_data}\")\n",
        "\n",
        "          index += 1\n",
        "  print(\"Batch time : \", time.time() - start)\n",
        "\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  return np.array(embedded), np.array(le.fit_transform(data[\"label1\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGiosHvf2UVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comme l'entrainement est tres long et qu'il y a beaucoup de donnees, on le fait en batch de \n",
        "# 10'000 qu'on enregistre dans des fichiers.\n",
        "if not incremental_treatment:\n",
        "  train_emb, train_labels_ = run_batch(train_data.head(10000))\n",
        "else:\n",
        "  i = 0\n",
        "  j = 10000\n",
        "  while i+j < len(train_data):\n",
        "      emb, labels = run_batch(train_data[i:i+j])\n",
        "      np.savetxt('train_emb_data{}.npy'.format(i), np.array(emb))\n",
        "      np.savetxt('train_emb_labels{}.npy'.format(i), np.array(labels))\n",
        "      i += j\n",
        "\n",
        "  # Combine les fichiers train_emb_data{i} en un seul fichier\n",
        "  i = 0\n",
        "  j = 10000\n",
        "  embs = None\n",
        "  labels = None\n",
        "  while True:\n",
        "      name1 = 'train_emb_data{}.npy'.format(i)\n",
        "      name2 = 'train_emb_labels{}.npy'.format(i)\n",
        "      print(\"Reading : \" + name1)\n",
        "      a = np.loadtxt(name1)\n",
        "      print(\"Reading : \" + name2)\n",
        "      b = np.loadtxt(name2)\n",
        "      if embs is None:\n",
        "          embs = a\n",
        "          labels = b\n",
        "      else:\n",
        "          embs = np.vstack((embs, a))\n",
        "          labels = np.vstack((labels, b))\n",
        "      print(i)\n",
        "      i += j\n",
        "  np.savetxt('{}_embs.npy'.format(i), np.array(embs))\n",
        "  np.savetxt('{}_labels.npy'.format(i), np.array(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKqLlK3i2UVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pretraite les donnees de n'ensemble de validation\n",
        "valid_emb, valid_labels_ = run_batch(validation_data.head(5000))\n",
        "np.savetxt('valid_emb_data.npy', np.array(valid_emb))\n",
        "np.savetxt('valid_emb_labels.npy', np.array(valid_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsUiY24V2UVm",
        "colab_type": "text"
      },
      "source": [
        "###  Chargement des plongements enregistrés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY-LpOMU2UVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if incremental_treatment:\n",
        "  train_emb = np.loadtxt(prefix + '140000_embs.npy')\n",
        "  train_labels_ = np.loadtxt(prefix + \"140000_labels.npy\").flatten()\n",
        "  valid_emb = np.loadtxt(prefix + 'valid_emb_data.npy')\n",
        "  valid_labels_ = np.loadtxt(prefix + 'valid_emb_labels.npy')\n",
        "\n",
        "# Transforme les labels en données catégoriques pour la prédiction\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_labels_)\n",
        "train_labels = np_utils.to_categorical(encoder.transform(train_labels_))\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(valid_labels_)\n",
        "valid_labels = np_utils.to_categorical(encoder.transform(valid_labels_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROfVm42KTXbz",
        "colab_type": "text"
      },
      "source": [
        "### Modele simple FeedForwad avec plongements bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfTA2WS22UV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model le plus simple, feed forward de 32 node\n",
        "model = Sequential([\n",
        "        Dense(32, input_shape=(len(train_emb[0]),), activation=\"sigmoid\"),\n",
        "        Dense(32, activation=\"sigmoid\"),\n",
        "        Dense(32, activation=\"sigmoid\"),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "model.compile(\n",
        "    optimizer=\"sgd\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "model.fit(\n",
        "    train_emb,\n",
        "    train_labels_,\n",
        "    epochs=4,\n",
        "    verbose=2,\n",
        "    batch_size=8,\n",
        "    validation_data=[valid_emb, valid_labels_])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30cYq-VuTjkF",
        "colab_type": "text"
      },
      "source": [
        "### Modele simple LSTM avec plongements bert "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INCOAmzS2UV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_b = Sequential()\n",
        "model_b.add(Bidirectional(LSTM(128), input_shape=(1, len(train_emb[0]))))\n",
        "model_b.add(Dense(3, activation=\"softmax\"))\n",
        "model_b.compile(\n",
        "    optimizer=\"sgd\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "model_b.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G2A5L2o2UV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model_b.fit(\n",
        "    train_emb.reshape((len(train_emb),1, len(train_emb[0]))),\n",
        "    train_labels,\n",
        "    epochs=3, \n",
        "    batch_size=32,\n",
        "    validation_data=[valid_emb.reshape((len(valid_emb),1, len(valid_emb[0]))), valid_labels])\n",
        "history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mA-HpX-2UWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def display_accuracy(model):\n",
        "    plt.plot(model.history.history['acc'], label=\"Training set\")\n",
        "    plt.plot(model.history.history['val_acc'],label=\"Validation set\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "display_accuracy(model_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKucvCfVTF07",
        "colab_type": "text"
      },
      "source": [
        "## Modèle Complexe : BERT + PoS / BERT + SRL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrHNoGyC2UWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt4ssarQ2UWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import sklearn\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import optim\n",
        "from tqdm import tqdm, trange\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, Normalizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SequentialSampler\n",
        "\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertConfig, DistilBertTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jptycm41Ofl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uncomment pour utiliser google drive. Sinon les fichiers doivent être dans le meme dossier que le notebook\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/inf8460_projet')\n",
        "# prefix = \"./inf8460_projet/My Drive/inf8460_projet/\"\n",
        "prefix = \"./\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm3Xn0rLQD9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Une version de BERT très standard. L'entraînement est assez long\n",
        "# bert_set = \"bert-base-uncased\"\n",
        "# tokenizer_class = BertTokenizer\n",
        "# bert_class = BertForSequenceClassification\n",
        "# bert_config_class = BertConfig\n",
        "\n",
        "# Une version de BERT _beaucoup_ plus rapide. Perte de ~3% en performance, gain de ~30% en temps d'entraînement\n",
        "bert_set = \"distilbert-base-uncased\"\n",
        "tokenizer_class = DistilBertTokenizer\n",
        "bert_class = DistilBertForSequenceClassification\n",
        "bert_config_class = DistilBertConfig\n",
        "\n",
        "# Détermine si on entraîne le modèle en utilisant les tags sémantiques et si on utilise uniquement BERT pour l'inférence\n",
        "with_srl = False\n",
        "with_tags = True\n",
        "\n",
        "batch_size = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUPggl1laXFX",
        "colab_type": "text"
      },
      "source": [
        "### Traitement des Données\n",
        "\n",
        "Le prétraitement !!pour le SRL!! prend _beaucoup_ de temps. \n",
        "\n",
        "Vous trouverez en suivant le lien suivant un fichier contenant 25'000 samples pré-traité pour srl : https://drive.google.com/open?id=11lB-5yiqRdYVKLMKFEhv12PDyeTjtN7- \n",
        "\n",
        "Placez ces données dans le même dossier que le notebook et allez directement à la section Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8Epb6weVrd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lecture des données d'entraînement\n",
        "# On met les differents sets dans des fichiers pour etre sur que cela soit coherent entre les differentes intances du notebook\n",
        "all_data = pd.read_csv(os.path.join(prefix, 'snli_train.csv'))\n",
        "all_train_data, all_valid_data = train_test_split(all_data, test_size=0.1)\n",
        "all_train_data, all_test_data = train_test_split(all_train_data, test_size=0.05)\n",
        "\n",
        "all_test_data.to_csv(prefix + '/test_samples.csv', index=False) # 5% de toutes les données\n",
        "all_valid_data.to_csv(prefix + '/valid_samples.csv', index=False) # 10% d e toutes les données\n",
        "all_train_data.to_csv(prefix + '/train_samples.csv', index=False) # 85% de toutes les données"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSHwoVqGkK47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Permet de choisir le nombre de données à traiter afin de faire des itérations rapides. None pour toutes les utiliser\n",
        "num_samples = None \n",
        "num_test = None\n",
        "\n",
        "# On lit les fichiers enregistré plus haut (toujours dans le but de garder le tout cohérent)\n",
        "all_test_data = pd.read_csv(os.path.join(prefix, 'test_samples.csv'))\n",
        "all_valid_data = pd.read_csv(os.path.join(prefix, 'valid_samples.csv'))\n",
        "all_train_data = pd.read_csv(os.path.join(prefix, 'train_samples.csv'))\n",
        "\n",
        "if num_samples is not None:\n",
        "  all_valid_data = all_valid_data[num_samples:num_samples+int(num_samples*0.1)]\n",
        "  all_train_data = all_train_data[num_samples:num_samples+int(num_samples*0.90)]\n",
        "if num_test is not None:\n",
        "  all_test_data = all_test_data.head(num_test)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "all_train_labels = np.array(label_encoder.fit_transform(all_train_data[\"label1\"])) \n",
        "all_valid_labels = np.array(label_encoder.fit_transform(all_valid_data[\"label1\"]))\n",
        "all_test_labels = np.array(label_encoder.fit_transform(all_test_data[\"label1\"]))\n",
        "\n",
        "print(\"{} Training samples\".format(len(all_train_data)))\n",
        "print(\"{} Validating samples\".format(len(all_valid_data)))\n",
        "print(\"{} Testing samples\".format(len(all_test_data)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bif16f5c1wwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/bert-base-srl-2019.06.17.tar.gz\")\n",
        "\n",
        "def get_semtags(tokens):\n",
        "  \"\"\" execute le Semantic role labeling sur l'entré tokenisé donnée. \"\"\"\n",
        "  if len(tokens) == 0:\n",
        "    return []\n",
        "  index_sep = np.nonzero(np.array(tokens)==\"[SEP]\")[0][0]\n",
        "  sentence1 = ' '.join(tokens[1:index_sep]) # From after [CLS] to before the first [SEP]\n",
        "  sentence2 = ' '.join(tokens[index_sep+1:-1]) # From after the first [SEP] to before the last [SEP]\n",
        "\n",
        "  t1 = get_sem(sentence1)\n",
        "  t2 = get_sem(sentence2)\n",
        "  return ['[CLS]', *t1, '[SEP]', *t2, '[SEP]']\n",
        "\n",
        "def get_sem(sentence):\n",
        "  \"\"\" execute le Semantic role labeling sur la phrase tokenisé donnée. \n",
        "      Retourne un ensemble de \"O\" (~=unknown) si aucune représentation n'est trouvée\n",
        "      En cas d'ambiguité, retourne la représentation trouvant le plus grand nombre d'étiquettes\"\"\"\n",
        "  l = [d['tags'] for d in predictor.predict(sentence=sentence)['verbs']]\n",
        "  if len(l) == 0:\n",
        "    return np.full((1, sentence.count(\" \") + 1), 'O').tolist()[0]\n",
        "  index_best = np.array([labels.count('O') for labels in l]).argmin()\n",
        "  return l[index_best]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij0hxmRZQUI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fonctions utilitaire pour faire du part of speech sur les WordPiece de bert\n",
        "def bert_token_to_regular(tokens):\n",
        "  \"\"\" Transforme les tokens pour bert de forme [\"sleep\", \"##ing\"] en tokens normal -> [\"sleeping\"] \"\"\"\n",
        "  out = []\n",
        "  for x in tokens:\n",
        "    if x[0:2] == \"##\":\n",
        "      out[-1] += x[2:]\n",
        "    else:\n",
        "      out.append(x)\n",
        "  return out\n",
        "\n",
        "def get_tags(tokens):\n",
        "  \"\"\" Etiquette les tokens \"\"\"\n",
        "  return [t for w,t in bert_token_to_regular(nltk.pos_tag(tokens))]\n",
        "\n",
        "def tags_to_bert(tags, bert_tokens):\n",
        "  \"\"\" Etiquette les tokens de bert a partir des tags normaux generés par bert_token_to_regular et get_tags \"\"\"\n",
        "  out = []\n",
        "  j = 0\n",
        "  for i, x in enumerate(bert_tokens):\n",
        "    if x[0:2] == \"##\":\n",
        "      out.append(out[-1])\n",
        "    elif x == \"[CLS]\" or x == \"[SEP]\":\n",
        "      out.append(\"[PAD]\")\n",
        "      j += 1\n",
        "    else:\n",
        "      out.append(tags[j])\n",
        "      j += 1\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzPLDB6QWpT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Formatte les données dans un format compatible avec BERT (tokenisation, conversion en input id, padding, masques d'attentions et format torch.tensor)\n",
        "# Inspiré du blog de Chris McCormick : https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "MAX_LEN= 128\n",
        "def format_bert(data, _tokenizer):\n",
        "  \"\"\" Formate des donnees dans un dataframe ayant les colonnes [sentence1] [sentence2] afin quelle puisse etre utiliser avec la lib hungingface \"\"\"\n",
        "\n",
        "  print(\"\\nTokenize les entrées\"); start = time.time()\n",
        "  tokenized_texts = [[\"[CLS]\", *_tokenizer.tokenize(sent1), \"[SEP]\", *_tokenizer.tokenize(sent2), \"[SEP]\"] for sent1, sent2 in zip(data['sentence1'], data['sentence2'])]\n",
        "  print(\"\\tTerminé : {} secondes\".format(time.time() - start))\n",
        "  \n",
        "  if with_tags:\n",
        "    if with_srl:\n",
        "      print(\"Etiquetage de labels sémantiques\"); start = time.time()\n",
        "      tagged_texts = [tags_to_bert(get_semtags(bert_token_to_regular(tokens)), tokens) for tokens in tokenized_texts]\n",
        "    else:\n",
        "      print(\"Etiquetage morphosyntaxique\"); start = time.time()\n",
        "      tagged_texts = [tags_to_bert(get_tags(tokens), tokens) for tokens in tokenized_texts]\n",
        "    print(\"\\tTerminé : {} secondes\".format(time.time() - start))\n",
        "  else:\n",
        "    tagged_texts = None\n",
        "\n",
        "  print(\"Pad les entrées et les transformes en ids pour bert\"); start = time.time()\n",
        "  input_ids = pad_sequences([_tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  input_ids = [_tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  print(\"\\tTerminé : {} secondes\".format(time.time() - start))\n",
        "\n",
        "  print(\"Créer les masques d'attention (1 si un mot est présent)\"); start = time.time()\n",
        "  attention_masks = []\n",
        "  for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "  print(\"\\tTerminé : {} secondes\".format(time.time() - start))\n",
        "  return input_ids, attention_masks, tagged_texts\n",
        "\n",
        "start = time.time()\n",
        "tokenizer = tokenizer_class.from_pretrained(bert_set, do_lower_case=True)\n",
        "\n",
        "# # Traiter les donnees train\n",
        "t_input_ids, t_attention_masks, t_semtags = format_bert(all_train_data, tokenizer)\n",
        "\n",
        "train_inputs = torch.tensor(t_input_ids)\n",
        "train_masks = torch.tensor(t_attention_masks)\n",
        "train_labels = torch.tensor(all_train_labels)\n",
        "\n",
        "\n",
        "# # Traiter les donnees validation\n",
        "v_input_ids, v_attention_masks, v_semtags = format_bert(all_valid_data, tokenizer)\n",
        "\n",
        "validation_inputs = torch.tensor(v_input_ids)\n",
        "validation_masks = torch.tensor(v_attention_masks)\n",
        "validation_labels = torch.tensor(all_valid_labels)\n",
        "\n",
        "\n",
        "# # Traiter les donnees de test\n",
        "te_input_ids, te_attention_masks, te_semtags= format_bert(all_test_data, tokenizer)\n",
        "\n",
        "testing_inputs = torch.tensor(te_input_ids)\n",
        "testing_masks = torch.tensor(te_attention_masks)\n",
        "testing_labels = torch.tensor(all_test_labels)\n",
        "\n",
        "# Transforme les semtags trouve precedemment en format categorique\n",
        "all_semtags = set()\n",
        "for semtags in t_semtags + v_semtags + te_semtags:\n",
        "  all_semtags = all_semtags.union(set(semtags))\n",
        "semtag_encoder = LabelEncoder()\n",
        "semtag_encoder.fit(list(all_semtags))\n",
        "\n",
        "def semtags_to_categorical(semtag_set):\n",
        "  for i, semtags in enumerate(semtag_set):\n",
        "    semtag_set[i] = semtag_encoder.transform(semtags)\n",
        "  semtag_set = pad_sequences(semtag_set, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  return semtag_set\n",
        "\n",
        "scaler = Normalizer(norm=\"max\")\n",
        "t_semtags = semtags_to_categorical(t_semtags)\n",
        "train_semtags = scaler.fit_transform(torch.tensor(t_semtags))\n",
        "\n",
        "v_semtags = semtags_to_categorical(v_semtags)\n",
        "validation_semtags = scaler.transform(torch.tensor(v_semtags))\n",
        "\n",
        "te_semtags = semtags_to_categorical(te_semtags)\n",
        "test_semtags = scaler.transform(torch.tensor(te_semtags))\n",
        "\n",
        "# Le traitement peut etre tres long. On sauvegarde la sortie dans des fichiers pour pouvoir iterer plus rapidement\n",
        "np.savetxt(os.path.join(prefix, 'train_inputs.csv'), train_inputs, delimiter=\",\")\n",
        "np.savetxt(os.path.join(prefix, 'train_masks.csv'), train_masks, delimiter=\",\")\n",
        "np.savetxt(os.path.join(prefix, 'train_labels.csv'), train_labels, delimiter=\",\")\n",
        "np.savetxt(os.path.join(prefix, 'train_semtags.csv'), train_semtags, delimiter=\",\")\n",
        "\n",
        "np.savetxt(os.path.join(prefix, 'validation_inputs.csv'), validation_inputs, delimiter=\",\")\n",
        "np.savetxt(os.path.join(prefix, 'validation_masks.csv'), validation_masks, delimiter=\",\")\n",
        "np.savetxt(os.path.join(prefix, 'validation_labels.csv'), validation_labels, delimiter=\",\")\n",
        "np.savetxt(os.path.join(prefix, 'validation_semtags.csv'), validation_semtags, delimiter=\",\")\n",
        "\n",
        "np.savetxt(os.path.join(prefix, 'testing_inputs.csv'), testing_inputs, delimiter=\",\")\n",
        "np.savetxt(os.path.join(prefix, 'testing_masks.csv'), testing_masks, delimiter=\",\")\n",
        "np.savetxt(os.path.join(prefix, 'testing_labels.csv'), testing_labels, delimiter=\",\")\n",
        "np.savetxt(os.path.join(prefix, 'test_semtags.csv'), test_semtags, delimiter=\",\")\n",
        "\n",
        "print(\"Terminé : {} secondes\".format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfkjTU5Uapjx",
        "colab_type": "text"
      },
      "source": [
        "### Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5QDNe10q5aA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chargement des données pré-traité (voir cellule ci-dessus)\n",
        "train_inputs = torch.tensor(np.loadtxt(os.path.join(prefix, 'train_inputs1.csv'), delimiter=\",\")).long()\n",
        "train_masks = torch.tensor(np.loadtxt(os.path.join(prefix, 'train_masks1.csv'), delimiter=\",\")).long()\n",
        "train_labels = torch.tensor(np.loadtxt(os.path.join(prefix, 'train_labels1.csv'), delimiter=\",\")).long()\n",
        "train_semtags = torch.tensor(np.loadtxt(os.path.join(prefix, 'train_semtags1.csv'), delimiter=\",\")).float()\n",
        "\n",
        "validation_inputs = torch.tensor(np.loadtxt(os.path.join(prefix, 'validation_inputs1.csv'), delimiter=\",\")).long()\n",
        "validation_masks = torch.tensor(np.loadtxt(os.path.join(prefix, 'validation_masks1.csv'), delimiter=\",\")).long()\n",
        "validation_labels = torch.tensor(np.loadtxt(os.path.join(prefix, 'validation_labels1.csv'), delimiter=\",\")).long()\n",
        "validation_semtags = torch.tensor(np.loadtxt(os.path.join(prefix, 'validation_semtags1.csv'), delimiter=\",\"))\n",
        "\n",
        "testing_inputs = torch.tensor(np.loadtxt(os.path.join(prefix, 'testing_inputs1.csv'), delimiter=\",\")).long()\n",
        "testing_masks = torch.tensor(np.loadtxt(os.path.join(prefix, 'testing_masks1.csv'), delimiter=\",\")).long()\n",
        "testing_labels = torch.tensor(np.loadtxt(os.path.join(prefix, 'testing_labels1.csv'), delimiter=\",\")).long()\n",
        "test_semtags = torch.tensor(np.loadtxt(os.path.join(prefix, 'test_semtags1.csv'), delimiter=\",\")).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EooUnz18tbeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creer les datasets\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_semtags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_semtags)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "testing_data = TensorDataset(testing_inputs, testing_masks, testing_labels, test_semtags)\n",
        "testing_sampler = SequentialSampler(testing_data)\n",
        "testing_dataloader = DataLoader(testing_data, sampler=testing_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud_t9sHlZu-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl7J89fYabne",
        "colab_type": "text"
      },
      "source": [
        "### Définition du Modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKIFpGWGe1E4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\" Modèle PyTorch permettant de lier BERT à un MLP/couche de neuronnes \"\"\"\n",
        "    def __init__(self, with_tags):\n",
        "        super(MLP, self).__init__()\n",
        "        self.with_tags = with_tags\n",
        "        config = bert_config_class.from_pretrained(bert_set)\n",
        "        config.output_hidden_states = True\n",
        "        config.num_labels = 3\n",
        "\n",
        "        self.pool = nn.AvgPool1d(256, stride=1024)\n",
        "        self.bert = bert_class.from_pretrained(bert_set, config=config)\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 3)\n",
        "        )\n",
        "        self.mlp_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def format_input(self, hidden_state_layer, tags):\n",
        "        \"\"\" Lie un couche caché de bert à un vecteur de tags unidimensionnel.\n",
        "            Utilise un algorithme de pooling (voir self.pool) pour transformer le layer en un vecteur 1d et le concatene avec les tags \"\"\"\n",
        "        b = self.pool(hidden_state_layer)\n",
        "        b = b.reshape((hidden_state_layer.shape[0], hidden_state_layer.shape[1])) \n",
        "        return torch.cat((b, tags.float()), 1)\n",
        "        \n",
        "    def forward(self, ids, attention_mask, tags, labels=None):\n",
        "        if labels is not None:\n",
        "          loss_bert, a, hidden_states = self.bert(ids, attention_mask=attention_mask, labels=labels)\n",
        "          if not self.with_tags:\n",
        "            return loss_bert\n",
        "          \n",
        "          mlp_inputs = self.format_input(hidden_states[-1], tags)\n",
        "          out_mlp = self.layers(mlp_inputs)\n",
        "          return loss_bert + self.mlp_loss(out_mlp, _labels)\n",
        "        else:\n",
        "          logits, hidden_states = self.bert(ids, attention_mask=attention_mask)\n",
        "          if not self.with_tags:\n",
        "            return logits\n",
        "          \n",
        "          mlp_inputs = self.format_input(hidden_states[-1], tags)\n",
        "          return self.layers(mlp_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHHhlcTDx3UP",
        "colab_type": "text"
      },
      "source": [
        "### Entrainement du Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAUcp-FQx2uX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseline = MLP(with_tags = False)\n",
        "model = baseline\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV-fPJSXx_bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Entrainement et validation de BERT avec notre corpus\n",
        "def accuracy(got, expected):\n",
        "  \"\"\" Calcule le pourcentage des labels prédits correctement \"\"\"\n",
        "  got_flattened = np.argmax(got, axis=1).flatten()\n",
        "  expected_flattened = expected.flatten()\n",
        "  return np.sum(got_flattened == expected_flattened) / len(expected_flattened)\n",
        "\n",
        "# optimizer/settings de BERT\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_parameters, lr=2e-5, correct_bias=False)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=len(train_data)*0.05, num_training_steps=len(train_data)*0.95)\n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "generations = 6\n",
        "for _ in trange(generations, desc=\"Generation\"):\n",
        "  # ==================== Training Step =======================\n",
        "  model.train()  \n",
        "  total_loss = 0\n",
        "  for group in train_dataloader:\n",
        "    group = tuple(t.to(device) for t in group) # Transfer le groupe sur le gpu\n",
        "    _input_ids, _input_mask, _labels, _tags = group\n",
        "\n",
        "    # Forward Pass\n",
        "    loss = model(_input_ids, attention_mask=_input_mask, tags=_tags, labels=_labels)\n",
        "\n",
        "    # Backpass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad() # Remet le gradient du loss/optimizer à Zero. Sinon ils s'accumulent à travers le groupes et pollue les résulats du backpass\n",
        "    total_loss += loss.item()\n",
        "    # ========================================================\n",
        "\n",
        "  # ==================== Validation Step =====================\n",
        "  model.eval()\n",
        "  total_valid_accuracy = 0\n",
        "  for group in validation_dataloader:\n",
        "    group = tuple(t.to(device) for t in group)\n",
        "    _input_ids, _input_mask, _labels, _tags = group\n",
        "\n",
        "    # Feedfoward : Utilise le logits comme prédiction\n",
        "    with torch.no_grad():\n",
        "        logits = model(_input_ids, attention_mask=_input_mask, tags=_tags)     \n",
        "    # Déplace le tout vers le CPU pour l'analyse\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = _labels.to('cpu').numpy()\n",
        "    total_valid_accuracy += accuracy(logits, label_ids)\n",
        "    # ========================================================  \n",
        "  print(\"Loss: {}\\tVal Acc: {}\".format(\n",
        "      total_loss/len(train_dataloader),\n",
        "      total_valid_accuracy/len(validation_dataloader)))\n",
        "\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "print(\"# Params : \", sum([np.prod(p.size()) for p in model_parameters]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NzTcBpB2IRS",
        "colab_type": "text"
      },
      "source": [
        "### Entrainement du Modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hww5MYLx5_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MLP(with_tags=with_tags)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZlQ7Pe-WpyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Entrainement et validation de BERT avec notre corpus\n",
        "def accuracy(got, expected):\n",
        "  \"\"\" Calcule le pourcentage des labels prédits correctement \"\"\"\n",
        "  got_flattened = np.argmax(got, axis=1).flatten()\n",
        "  expected_flattened = expected.flatten()\n",
        "  return np.sum(got_flattened == expected_flattened) / len(expected_flattened)\n",
        "\n",
        "# optimizer/settings de BERT\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_parameters, lr=2e-5, correct_bias=False)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=len(train_data)*0.05, num_training_steps=len(train_data)*0.95)\n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "generations = 6\n",
        "for _ in trange(generations, desc=\"Generation\"):\n",
        "  # ==================== Training Step =======================\n",
        "  model.train()  \n",
        "  total_loss = 0\n",
        "  for group in train_dataloader:\n",
        "    group = tuple(t.to(device) for t in group) # Transfer le groupe sur le gpu\n",
        "    _input_ids, _input_mask, _labels, _tags = group\n",
        "\n",
        "    # Forward Pass\n",
        "    loss = model(_input_ids, attention_mask=_input_mask, tags=_tags, labels=_labels)\n",
        "\n",
        "    # Backpass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad() # Remet le gradient du loss/optimizer à Zero. Sinon ils s'accumulent à travers le groupes et pollue les résulats du backpass\n",
        "    total_loss += loss.item()\n",
        "    # ========================================================\n",
        "\n",
        "  # ==================== Validation Step =====================\n",
        "  model.eval()\n",
        "  total_valid_accuracy = 0\n",
        "  for group in validation_dataloader:\n",
        "    group = tuple(t.to(device) for t in group)\n",
        "    _input_ids, _input_mask, _labels, _tags = group\n",
        "\n",
        "    # Feedfoward : Utilise le logits comme prédiction\n",
        "    with torch.no_grad():\n",
        "        logits = model(_input_ids, attention_mask=_input_mask, tags=_tags)     \n",
        "    # Déplace le tout vers le CPU pour l'analyse\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = _labels.to('cpu').numpy()\n",
        "    total_valid_accuracy += accuracy(logits, label_ids)\n",
        "    # ========================================================  \n",
        "  print(\"Loss: {}\\tVal Acc: {}\".format(\n",
        "      total_loss/len(train_dataloader),\n",
        "      total_valid_accuracy/len(validation_dataloader)))\n",
        "\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "print(\"# Params : \", sum([np.prod(p.size()) for p in model_parameters]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRUmxXhLALYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Enregistre le modèle dans un fichier. Permet de par la suit re-charger ce même modèle (voir cellule ci-dessous)\n",
        "\n",
        "date_time = datetime.now().strftime(\"%m-%d-%Y_%Hh%Mm%Ss\")\n",
        "torch.save(model, prefix + \"model_{}.pth\".format(date_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pUHr9ehMzJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Charge le modèle à partir d'un fichier. Soit pour continuer l'entrainement, soit pour générer les prédictions de test.\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# n_gpu = torch.cuda.device_count()\n",
        "# torch.cuda.get_device_name(0)\n",
        "\n",
        "# model = torch.load(prefix + \"model_12-09-2019_01h00m52s.pth\")\n",
        "# model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RruQiPs5aDdj",
        "colab_type": "text"
      },
      "source": [
        "### Validation du Modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXzFMa_H8-Qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Entrainement et validation de BERT avec des donnees qu'il n'a jamais vues\n",
        "import time\n",
        "start = time.time()\n",
        "# ==================== Test Step =====================\n",
        "model.eval()\n",
        "num_steps, total_accuracy = 0, 0\n",
        "all_labels = None\n",
        "for group in testing_dataloader:\n",
        "  group = tuple(t.to(device) for t in group)\n",
        "  _input_ids, _input_mask, _labels, _tags = group\n",
        "\n",
        "  # Feedfoward : Utilise le logits comme prédiction\n",
        "  with torch.no_grad():\n",
        "      logits = model(_input_ids, attention_mask=_input_mask, tags=_tags)     \n",
        "  # Déplace le tout vers le CPU pour l'analyse\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = _labels.to('cpu').numpy()\n",
        "  pred_labels = np.argmax(logits, axis=1).flatten()\n",
        "  total_accuracy += sklearn.metrics.accuracy_score(pred_labels, label_ids)\n",
        "  if all_labels is None:\n",
        "    all_labels = pred_labels\n",
        "  else:\n",
        "    all_labels = np.hstack((all_labels, pred_labels))\n",
        "\n",
        "  num_steps += 1\n",
        "  # ========================================================  \n",
        "print(\"Val Acc: {}\".format(total_accuracy/num_steps))\n",
        "print(\"Took : {} seconds\".format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvGTyM9cPcQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copié de : https://matplotlib.org/3.1.1/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "def heatmap(data, row_labels, col_labels, ax=None,\n",
        "            cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
        "    \"\"\"\n",
        "    Create a heatmap from a numpy array and two lists of labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data\n",
        "        A 2D numpy array of shape (N, M).\n",
        "    row_labels\n",
        "        A list or array of length N with the labels for the rows.\n",
        "    col_labels\n",
        "        A list or array of length M with the labels for the columns.\n",
        "    ax\n",
        "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
        "        not provided, use current axes or create a new one.  Optional.\n",
        "    cbar_kw\n",
        "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
        "    cbarlabel\n",
        "        The label for the colorbar.  Optional.\n",
        "    **kwargs\n",
        "        All other arguments are forwarded to `imshow`.\n",
        "    \"\"\"\n",
        "\n",
        "    if not ax:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    # Plot the heatmap\n",
        "    im = ax.imshow(data, **kwargs)\n",
        "\n",
        "    # Create colorbar\n",
        "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
        "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
        "\n",
        "    # We want to show all ticks...\n",
        "    ax.set_xticks(np.arange(data.shape[1]))\n",
        "    ax.set_yticks(np.arange(data.shape[0]))\n",
        "    # ... and label them with the respective list entries.\n",
        "    ax.set_xticklabels(col_labels)\n",
        "    ax.set_yticklabels(row_labels)\n",
        "\n",
        "    # Let the horizontal axes labeling appear on top.\n",
        "    ax.tick_params(top=True, bottom=False,\n",
        "                   labeltop=True, labelbottom=False)\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Turn spines off and create white grid.\n",
        "    for edge, spine in ax.spines.items():\n",
        "        spine.set_visible(False)\n",
        "\n",
        "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
        "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
        "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
        "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
        "\n",
        "    return im, cbar\n",
        "\n",
        "\n",
        "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
        "                     textcolors=[\"black\", \"white\"],\n",
        "                     threshold=None, **textkw):\n",
        "    \"\"\"\n",
        "    A function to annotate a heatmap.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    im\n",
        "        The AxesImage to be labeled.\n",
        "    data\n",
        "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
        "    valfmt\n",
        "        The format of the annotations inside the heatmap.  This should either\n",
        "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
        "        `matplotlib.ticker.Formatter`.  Optional.\n",
        "    textcolors\n",
        "        A list or array of two color specifications.  The first is used for\n",
        "        values below a threshold, the second for those above.  Optional.\n",
        "    threshold\n",
        "        Value in data units according to which the colors from textcolors are\n",
        "        applied.  If None (the default) uses the middle of the colormap as\n",
        "        separation.  Optional.\n",
        "    **kwargs\n",
        "        All other arguments are forwarded to each call to `text` used to create\n",
        "        the text labels.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(data, (list, np.ndarray)):\n",
        "        data = im.get_array()\n",
        "\n",
        "    # Normalize the threshold to the images color range.\n",
        "    if threshold is not None:\n",
        "        threshold = im.norm(threshold)\n",
        "    else:\n",
        "        threshold = im.norm(data.max())/2.\n",
        "\n",
        "    # Set default alignment to center, but allow it to be\n",
        "    # overwritten by textkw.\n",
        "    kw = dict(horizontalalignment=\"center\",\n",
        "              verticalalignment=\"center\")\n",
        "    kw.update(textkw)\n",
        "\n",
        "    # Get the formatter in case a string is supplied\n",
        "    if isinstance(valfmt, str):\n",
        "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
        "\n",
        "    # Loop over the data and create a `Text` for each \"pixel\".\n",
        "    # Change the text's color depending on the data.\n",
        "    texts = []\n",
        "    for i in range(data.shape[0]):\n",
        "        for j in range(data.shape[1]):\n",
        "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
        "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
        "            texts.append(text)\n",
        "\n",
        "    return texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx0btKf9JeiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "total_contradiction = len([x for x in all_test_data[\"label1\"] if x == \"contradiction\"])\n",
        "total_entailment = len([x for x in all_test_data[\"label1\"] if x == \"entailment\"])\n",
        "total_neutral = len([x for x in all_test_data[\"label1\"] if x == \"neutral\"])\n",
        "\n",
        "print(\"Total Contradiction : \", total_contradiction)\n",
        "print(\"Total Entailment : \", total_entailment)\n",
        "print(\"Total Neutral : \", total_neutral)\n",
        "\n",
        "string_labels = label_encoder.inverse_transform(all_labels)\n",
        "pred_contradiction = len([x for x in string_labels if x == \"contradiction\"])\n",
        "pred_entailment = len([x for x in string_labels if x == \"entailment\"])\n",
        "pred_neutral = len([x for x in string_labels if x == \"neutral\"])\n",
        "\n",
        "print(\"\\nPred Contradiction : \", pred_contradiction)\n",
        "print(\"Pred Entailment : \", pred_entailment)\n",
        "print(\"Pred Neutral : \", pred_neutral)\n",
        "\n",
        "d = {'contradiction': defaultdict(int), 'entailment': defaultdict(int), 'neutral': defaultdict(int)}\n",
        "for i in range(len(string_labels)):\n",
        "  d[all_test_data[\"label1\"][i]][string_labels[i]] += 1\n",
        "data = np.array([[d['contradiction']['contradiction']/total_contradiction, d['contradiction']['entailment']/total_contradiction, d['contradiction']['neutral']/total_contradiction],\n",
        "        [d['entailment']['contradiction']/total_entailment, d['entailment']['entailment']/total_entailment, d['entailment']['neutral']/total_entailment],\n",
        "        [d['neutral']['contradiction']/total_neutral, d['neutral']['entailment']/total_neutral, d['neutral']['neutral']/total_neutral]])\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV-rn-1iPd68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "baaa33d2-1544-47c3-8dac-5f3d0a66df93"
      },
      "source": [
        "# Genere un graphique permettant la comparaison des performances selon les différences classes\n",
        "# Heatmap de prediction vs réalité par classe\n",
        "fig, ax = plt.subplots()\n",
        "headers = [\"Contradiction\", \"Entailment\", \"Neutral\"]\n",
        "im, cbar = heatmap(data, headers, headers, ax=ax,\n",
        "                   cmap=\"YlGn\", cbarlabel=\"\")\n",
        "texts = annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
        "ax.set_title(\"Distribution des prédictions selon leurs vraies classes\")\n",
        "ax.set_ylabel(\"Classe Attendue\")\n",
        "ax.set_xlabel(\"Classe Prédite\")\n",
        "fig.tight_layout()\n",
        "# fig.set_size_inches(18.5, 10.5)\n",
        "fig.savefig(prefix + \"pred_vs_real.png\", dpi=100)\n",
        "plt.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWMAAAEYCAYAAACJJ5fjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv3BO1PH5cEA5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egy-WnMIaQbs",
        "colab_type": "text"
      },
      "source": [
        "### Generation des résultats sur snli_test.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGPbOM12aPZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lecture des données d'entraînement\n",
        "start = time.time()\n",
        "f_test_data = pd.read_csv(os.path.join(prefix, 'snli_test.csv'))\n",
        "test_ids, test_masks, test_semtags = format_bert(f_test_data, tokenizer)\n",
        "\n",
        "test_tags = tags_to_categorical(test_semtags)\n",
        "\n",
        "test_tags = torch.tensor(test_tags)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "\n",
        "t_data = TensorDataset(test_ids, test_masks, test_tags)\n",
        "t_sampler = SequentialSampler(t_data)\n",
        "test_dataloader = DataLoader(t_data, sampler=t_sampler, batch_size=batch_size)\n",
        "print(\"Terminé : {} secondes\".format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHk2nVPYbVIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "# ==================== Predicting Step =====================\n",
        "model.eval()\n",
        "num_steps, total_accuracy = 0, 0\n",
        "all_labels = None\n",
        "for group in test_dataloader:\n",
        "  group = tuple(t.to(device) for t in group)\n",
        "  _input_ids, _input_mask, _tags = group\n",
        "  with torch.no_grad():\n",
        "      logits = model(_input_ids, attention_mask=_input_mask, tags=_tags)     \n",
        "  # Déplace le tout vers le CPU pour utilisation ultérieure\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  labels = np.argmax(logits, axis=1).flatten()\n",
        "  if all_labels is None:\n",
        "    all_labels = labels\n",
        "  else:\n",
        "    all_labels = np.hstack((all_labels, labels))\n",
        "  num_steps += 1\n",
        "  # ========================================================  \n",
        "print(\"Took : {} seconds\".format(time.time() - start))\n",
        "f_test_data[\"label1\"] = label_encoder.inverse_transform(all_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gYfVpnpFXmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Écrit le fichier de soumission\n",
        "submission = f_test_data[[\"id\", \"label1\"]]\n",
        "submission.to_csv(prefix + '/submission.csv', index=False)\n",
        "submission.head()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}